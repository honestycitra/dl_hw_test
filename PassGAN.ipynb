{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PassGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPp/VcKGTkVq+66H1uEtwkG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honestycitra/dl_hw_test/blob/main/PassGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PassGAN**\n",
        "\n",
        "Source: https://github.com/DSC-UI-SRIN/Introduction-to-GAN/blob/master/4%20-%20Applications%20of%20GANs/PassGAN.ipynb"
      ],
      "metadata": {
        "id": "JPUn9A9Ij7n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Introduction**\n",
        "\n",
        "##**Abstract**:\n",
        "\n",
        "State-of-the-art password guessing tools, such as HashCat and John the Ripper, enable users to check billions of passwords per second against password hashes. In addition to performing straightforward dictionary attacks, these tools can expand password dictionaries using password generation rules, such as concatenation of words (e.g., \"password123456\") and leet speak (e.g., \"password\" becomes \"p4s5w0rd\"). Although these rules work well in practice, expanding them to model further passwords is a laborious task that requires specialized expertise. To address this issue, in this paper we introduce PassGAN, a novel approach that replaces human-generated password rules with theory-grounded machine learning algorithms. Instead of relying on manual password analysis, PassGAN uses a Generative Adversarial Network (GAN) to autonomously learn the distribution of real passwords from actual password leaks, and to generate high-quality password guesses. Our experiments show that this approach is very promising. When we evaluated PassGAN on two large password datasets, we were able to surpass rule-based and state-of-the-art machine learning password guessing tools. However, in contrast with the other tools, PassGAN achieved this result without any a-priori knowledge on passwords or common password structures. Additionally, when we combined the output of PassGAN with the output of HashCat, we were able to match 51%-73% more passwords than with HashCat alone. This is remarkable, because it shows that PassGAN can autonomously extract a considerable number of password properties that current state-of-the art rules do not encode."
      ],
      "metadata": {
        "id": "0APyhnqqkDeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequest**"
      ],
      "metadata": {
        "id": "t24b314kkSn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyGl-_cJj4a9",
        "outputId": "e03779b8-34e3-4c22-a58f-d8922cecba24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import All prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import os\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ROOT = \"password/\"\n",
        "\n",
        "# Make dir if no exist\n",
        "if not os.path.exists(ROOT):\n",
        "    os.makedirs(ROOT)\n",
        "\n",
        "# Download Library\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/DSC-UI-SRIN/Introduction-to-GAN/master/4%20-%20Applications%20of%20GANs/password/datasets.py\n",
        "\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/DSC-UI-SRIN/Introduction-to-GAN/master/4%20-%20Applications%20of%20GANs/password/utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4pKqE8YkXyA",
        "outputId": "ef377faf-9dd3-4dff-ca14-514ebeaa4b2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6417  100  6417    0     0  66843      0 --:--:-- --:--:-- --:--:-- 66843\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4331  100  4331    0     0  77339      0 --:--:-- --:--:-- --:--:-- 77339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "eU0U2d60ka2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "batch_size = 100\n",
        "\n",
        "# Rockyou Dataset\n",
        "\n",
        "train_dataset = datasets.Rockyou(root=ROOT, train=True, download=True, input_size=(10,0), tokenize=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pCNWQp1kek8",
        "outputId": "eda32cee-b742-41a4-eab3-b33f966d3466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for the the first time...\n",
            "Pre-process data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "21315685it [00:36, 584442.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make Charmap..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:00<00:00, 776441.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter data and convert to discrete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21315685/21315685 [01:23<00:00, 256129.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 21315685 lines in dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_data.shape)"
      ],
      "metadata": {
        "id": "GJ8Jcr7ekgcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Building Model**"
      ],
      "metadata": {
        "id": "ZYV7h5Sbkh-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, functional\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(dim, dim, padding=kernel_size//2, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(dim, dim, padding=kernel_size//2, kernel_size=kernel_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = (self.model(input_data))\n",
        "        return input_data + output"
      ],
      "metadata": {
        "id": "i4XR0-R6klXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generator Model**"
      ],
      "metadata": {
        "id": "2zGGnUKAknhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, seq_len, layer_dim, z_dim, char_len):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.layer_dim = layer_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.char_len = char_len\n",
        "\n",
        "        self.linear = nn.Linear(self.z_dim, self.seq_len*self.layer_dim)\n",
        "\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "        )\n",
        "        self.conv = nn.Conv1d(self.layer_dim, self.char_len, kernel_size=1)\n",
        "\n",
        "    def softmax(self, logits, num_classes):\n",
        "        logits = logits.reshape(-1, num_classes)\n",
        "        logits = logits.softmax(1)\n",
        "        return logits.reshape(-1, self.seq_len, self.char_len)\n",
        "\n",
        "    def forward(self, z_input):\n",
        "        output = self.linear(z_input)\n",
        "        output = output.view(-1, self.layer_dim, self.seq_len)\n",
        "        output = self.res_blocks(output)\n",
        "        output = self.conv(output)\n",
        "        output = output.permute([0, 2, 1])\n",
        "        output = self.softmax(output, self.char_len)\n",
        "        return output"
      ],
      "metadata": {
        "id": "ct0PraIAkpwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discriminator Model**"
      ],
      "metadata": {
        "id": "E9hgQz00ks10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, seq_len, layer_dim, char_len):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.layer_dim = layer_dim\n",
        "        self.char_len = char_len\n",
        "\n",
        "        self.conv = nn.Conv1d(self.char_len, self.layer_dim, kernel_size=1)\n",
        "\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "            ResBlock(self.layer_dim),\n",
        "        )\n",
        "        self.linear = nn.Linear(self.seq_len*self.layer_dim, 1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = input_data.permute([0, 2, 1])\n",
        "        output = self.conv(output)\n",
        "        output = self.res_blocks(output)\n",
        "        output = output.view(-1, self.layer_dim*self.seq_len)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "D-mQWd2akvpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Network**"
      ],
      "metadata": {
        "id": "ZObdwGlokx2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build network\n",
        "z_dim = 128\n",
        "seq_len = 10\n",
        "layer_dim = 128\n",
        "\n",
        "\n",
        "G = Generator(seq_len, layer_dim, z_dim, len(train_dataset.class_to_idx)).to(device)\n",
        "D = Discriminator(seq_len, layer_dim, len(train_dataset.class_to_idx)).to(device)"
      ],
      "metadata": {
        "id": "nW467hA0kz86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(G, D)"
      ],
      "metadata": {
        "id": "Icbe_cwjk2RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Process**"
      ],
      "metadata": {
        "id": "RsZpNqgsk7oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradient Penalty**"
      ],
      "metadata": {
        "id": "6jg6FrNyk_D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_penalty(D, real_data, fake_data):\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(\n",
        "        np.random.random((real_data.size(0), 1, 1)))\n",
        "\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "    d_interpolates = D(interpolates.requires_grad_(True))\n",
        "    fake = Tensor(real_data.shape[0], 1).fill_(1.0)\n",
        "\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    grads = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    grads = grads.reshape(grads.size(0), -1)\n",
        "    grad_penalty = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return grad_penalty"
      ],
      "metadata": {
        "id": "8LcLvGtVk-MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss weight for gradient penalty\n",
        "lambda_gp = 10\n",
        "\n",
        "# optimizer\n",
        "lr = 1e-4\n",
        "n_critic =  5\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "\n",
        "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "metadata": {
        "id": "JluXkoa2lDDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logdir = './runs'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "\n",
        "writer = SummaryWriter(logdir)"
      ],
      "metadata": {
        "id": "bi5VJo1xlE0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "metadata": {
        "id": "uoFq9pswlGTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_generated_data(samples, iters, tag=\"result\"):\n",
        "    \"\"\"\n",
        "    this function used for check the result of generator network and save it to tensorboard\n",
        "    :param samples(dict): samples of input network\n",
        "    :param tag: save the output to tensorboard log wit tag\n",
        "    :param iters: global iteration counts for tensorboard logging\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        inv_charmap = train_dataset.idx_to_class\n",
        "\n",
        "        samples = G(samples)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            samples = samples.cpu().numpy()\n",
        "        else:\n",
        "            samples = samples.numpy()\n",
        "\n",
        "        samples = np.argmax(samples, axis=2)\n",
        "\n",
        "        decoded_samples = []\n",
        "        for i in range(len(samples)):\n",
        "            decoded = []\n",
        "            for j in range(len(samples[i])):\n",
        "                decoded.append(inv_charmap[samples[i][j]])\n",
        "            decoded_samples.append(\"\".join(decoded).replace('`', \"\"))\n",
        "        # print(\", \".join(decoded_samples))\n",
        "        writer.add_text(tag, \", \".join(decoded_samples), iters)"
      ],
      "metadata": {
        "id": "9JI6mEujlMgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "list_loss_D = []\n",
        "list_loss_G = []\n",
        "fixed_z = Variable(Tensor(np.random.normal(0, 1, (10, z_dim))))\n",
        "for epoch in range(epochs):\n",
        "    for i, (X, _) in enumerate(train_loader):\n",
        "        # Configure input\n",
        "        real_data = Variable(X.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (real_data.shape[0], z_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_data = G(z).detach()\n",
        "\n",
        "        # Gradient penalty\n",
        "        gradient_penalty = compute_gradient_penalty(D, real_data.data, fake_data.data)\n",
        "\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(D(real_data)) + torch.mean(D(fake_data)) + lambda_gp * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_data = G(z)\n",
        "            # Adversarial loss\n",
        "            g_loss = -torch.mean(D(gen_data))\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            list_loss_D.append(d_loss.item())\n",
        "            list_loss_G.append(g_loss.item())\n",
        "        \n",
        "        if i % 300 == 0:\n",
        "            print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch, epochs, i, len(train_loader), d_loss.item(), g_loss.item()))\n",
        "            writer.add_scalar('G_loss', g_loss.item(), epoch * len(train_loader) + i)\n",
        "            writer.add_scalar('D_loss', d_loss.item(), epoch * len(train_loader) + i)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        check_generated_data(fixed_z, tag=\"result_{}\".format(epoch), iters=epoch * len(train_loader) + i)"
      ],
      "metadata": {
        "id": "qufFasLHlR8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}